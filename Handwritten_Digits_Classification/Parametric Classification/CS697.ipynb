{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optical Recognition of Handwritten Digits Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Data\n",
    "1. Data Set Characteristics: Multivariate\n",
    "2. Number of Instances:\n",
    "\ta) optdigits_69_N200.tra for Training\tb) optdigits_69.tes for Testing\t\t\n",
    "3. Attribute Characteristics: Integer\n",
    "4. Number of Attributes: 64 inputs+1 class attribute\n",
    "5. Associated Tasks: Classification\n",
    "6. Missing Values? No\n",
    "7. For Each Attribute:\n",
    "\ta) All input attributes are integers in the range 0..16.\n",
    "\tb) The last attribute is the class code 0..9. We are using 6 and 9 only.\n",
    "8. Class Distribution\n",
    "\ta) Class:\tNo of examples in training set\n",
    "\t6:  100\n",
    "\t9:  100\n",
    "\tb) Class: No of examples in testing set\n",
    "\t6:  181\n",
    "\t9:  180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "from math import sqrt\n",
    "from math import pi\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from required files\n",
    "def load_data(filename):\n",
    "    with open(filename, \"r\") as file:\n",
    "        data_string = file.read()\n",
    "    data = io.StringIO(data_string)\n",
    "    colnames = [i for i in range(1,66)]\n",
    "    df = pd.read_csv(data, index_col=False, names=colnames, sep=\",\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df = load_data(\"optdigits_69_N200.tra\")\n",
    "test_df = load_data(\"optdigits_69.tes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200.0</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>2.970000</td>\n",
       "      <td>10.690000</td>\n",
       "      <td>9.805000</td>\n",
       "      <td>3.445000</td>\n",
       "      <td>1.225000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.070000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>3.155000</td>\n",
       "      <td>10.680000</td>\n",
       "      <td>13.520000</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>4.035000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>7.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.240393</td>\n",
       "      <td>3.566891</td>\n",
       "      <td>4.230055</td>\n",
       "      <td>4.627531</td>\n",
       "      <td>5.050198</td>\n",
       "      <td>3.454569</td>\n",
       "      <td>0.471046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.162982</td>\n",
       "      <td>...</td>\n",
       "      <td>1.403289</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.207824</td>\n",
       "      <td>3.567155</td>\n",
       "      <td>4.379899</td>\n",
       "      <td>3.342974</td>\n",
       "      <td>5.541982</td>\n",
       "      <td>4.564862</td>\n",
       "      <td>0.845384</td>\n",
       "      <td>1.503764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>12.750000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1           2           3           4           5           6   \\\n",
       "count  200.0  200.000000  200.000000  200.000000  200.000000  200.000000   \n",
       "mean     0.0    0.050000    2.970000   10.690000    9.805000    3.445000   \n",
       "std      0.0    0.240393    3.566891    4.230055    4.627531    5.050198   \n",
       "min      0.0    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.0    0.000000    0.000000    8.000000    7.000000    0.000000   \n",
       "50%      0.0    0.000000    1.000000   12.000000   11.000000    1.000000   \n",
       "75%      0.0    0.000000    5.000000   14.000000   13.000000    5.000000   \n",
       "max      0.0    2.000000   15.000000   16.000000   16.000000   16.000000   \n",
       "\n",
       "               7           8      9           10  ...          56     57  \\\n",
       "count  200.000000  200.000000  200.0  200.000000  ...  200.000000  200.0   \n",
       "mean     1.225000    0.065000    0.0    1.070000  ...    0.475000    0.0   \n",
       "std      3.454569    0.471046    0.0    2.162982  ...    1.403289    0.0   \n",
       "min      0.000000    0.000000    0.0    0.000000  ...    0.000000    0.0   \n",
       "25%      0.000000    0.000000    0.0    0.000000  ...    0.000000    0.0   \n",
       "50%      0.000000    0.000000    0.0    0.000000  ...    0.000000    0.0   \n",
       "75%      0.000000    0.000000    0.0    1.000000  ...    0.000000    0.0   \n",
       "max     16.000000    4.000000    0.0   11.000000  ...   12.000000    0.0   \n",
       "\n",
       "               58          59          60          61          62          63  \\\n",
       "count  200.000000  200.000000  200.000000  200.000000  200.000000  200.000000   \n",
       "mean     0.045000    3.155000   10.680000   13.520000   10.500000    4.035000   \n",
       "std      0.207824    3.567155    4.379899    3.342974    5.541982    4.564862   \n",
       "min      0.000000    0.000000    0.000000    1.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.000000    8.000000   12.750000    7.000000    0.000000   \n",
       "50%      0.000000    2.000000   11.500000   15.000000   12.500000    2.000000   \n",
       "75%      0.000000    6.000000   14.000000   16.000000   15.000000    7.000000   \n",
       "max      1.000000   13.000000   16.000000   16.000000   16.000000   16.000000   \n",
       "\n",
       "               64          65  \n",
       "count  200.000000  200.000000  \n",
       "mean     0.170000    7.500000  \n",
       "std      0.845384    1.503764  \n",
       "min      0.000000    6.000000  \n",
       "25%      0.000000    6.000000  \n",
       "50%      0.000000    7.500000  \n",
       "75%      0.000000    9.000000  \n",
       "max      7.000000    9.000000  \n",
       "\n",
       "[8 rows x 65 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>361.0</td>\n",
       "      <td>361.000000</td>\n",
       "      <td>361.000000</td>\n",
       "      <td>361.000000</td>\n",
       "      <td>361.000000</td>\n",
       "      <td>361.000000</td>\n",
       "      <td>361.000000</td>\n",
       "      <td>361.000000</td>\n",
       "      <td>361.0</td>\n",
       "      <td>361.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>361.00000</td>\n",
       "      <td>361.0</td>\n",
       "      <td>361.000000</td>\n",
       "      <td>361.000000</td>\n",
       "      <td>361.000000</td>\n",
       "      <td>361.000000</td>\n",
       "      <td>361.000000</td>\n",
       "      <td>361.000000</td>\n",
       "      <td>361.000000</td>\n",
       "      <td>361.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.072022</td>\n",
       "      <td>3.404432</td>\n",
       "      <td>11.498615</td>\n",
       "      <td>10.418283</td>\n",
       "      <td>3.689751</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.060942</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.229917</td>\n",
       "      <td>...</td>\n",
       "      <td>0.31856</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044321</td>\n",
       "      <td>3.576177</td>\n",
       "      <td>11.362881</td>\n",
       "      <td>14.121884</td>\n",
       "      <td>10.975069</td>\n",
       "      <td>3.290859</td>\n",
       "      <td>0.074792</td>\n",
       "      <td>7.495845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333864</td>\n",
       "      <td>3.831504</td>\n",
       "      <td>3.752869</td>\n",
       "      <td>4.599468</td>\n",
       "      <td>4.729121</td>\n",
       "      <td>2.782884</td>\n",
       "      <td>0.807361</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.329710</td>\n",
       "      <td>...</td>\n",
       "      <td>1.10851</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.231486</td>\n",
       "      <td>3.746167</td>\n",
       "      <td>3.804041</td>\n",
       "      <td>3.144942</td>\n",
       "      <td>4.783878</td>\n",
       "      <td>3.950548</td>\n",
       "      <td>0.431344</td>\n",
       "      <td>1.502076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1           2           3           4           5           6   \\\n",
       "count  361.0  361.000000  361.000000  361.000000  361.000000  361.000000   \n",
       "mean     0.0    0.072022    3.404432   11.498615   10.418283    3.689751   \n",
       "std      0.0    0.333864    3.831504    3.752869    4.599468    4.729121   \n",
       "min      0.0    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.0    0.000000    0.000000   10.000000    7.000000    0.000000   \n",
       "50%      0.0    0.000000    2.000000   13.000000   12.000000    1.000000   \n",
       "75%      0.0    0.000000    6.000000   14.000000   14.000000    6.000000   \n",
       "max      0.0    4.000000   15.000000   16.000000   16.000000   16.000000   \n",
       "\n",
       "               7           8      9           10  ...         56     57  \\\n",
       "count  361.000000  361.000000  361.0  361.000000  ...  361.00000  361.0   \n",
       "mean     0.789474    0.060942    0.0    1.229917  ...    0.31856    0.0   \n",
       "std      2.782884    0.807361    0.0    2.329710  ...    1.10851    0.0   \n",
       "min      0.000000    0.000000    0.0    0.000000  ...    0.00000    0.0   \n",
       "25%      0.000000    0.000000    0.0    0.000000  ...    0.00000    0.0   \n",
       "50%      0.000000    0.000000    0.0    0.000000  ...    0.00000    0.0   \n",
       "75%      0.000000    0.000000    0.0    1.000000  ...    0.00000    0.0   \n",
       "max     16.000000   15.000000    0.0   12.000000  ...    8.00000    0.0   \n",
       "\n",
       "               58          59          60          61          62          63  \\\n",
       "count  361.000000  361.000000  361.000000  361.000000  361.000000  361.000000   \n",
       "mean     0.044321    3.576177   11.362881   14.121884   10.975069    3.290859   \n",
       "std      0.231486    3.746167    3.804041    3.144942    4.783878    3.950548   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.000000   10.000000   14.000000    8.000000    0.000000   \n",
       "50%      0.000000    2.000000   12.000000   15.000000   12.000000    1.000000   \n",
       "75%      0.000000    6.000000   14.000000   16.000000   15.000000    6.000000   \n",
       "max      2.000000   15.000000   16.000000   16.000000   16.000000   16.000000   \n",
       "\n",
       "               64          65  \n",
       "count  361.000000  361.000000  \n",
       "mean     0.074792    7.495845  \n",
       "std      0.431344    1.502076  \n",
       "min      0.000000    6.000000  \n",
       "25%      0.000000    6.000000  \n",
       "50%      0.000000    6.000000  \n",
       "75%      0.000000    9.000000  \n",
       "max      6.000000    9.000000  \n",
       "\n",
       "[8 rows x 65 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the digits (few instances from both the classes): each image is 8x8 pixels\n",
    "x_train = np.array(train_df.loc[:,1:64])\n",
    "x_train = x_train.reshape(200, 8, 8)\n",
    "fig = plt.figure(figsize=(6, 6))  # figure size in inches\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "for i, j in zip([0,1,2,3,196,197,198,199],range(8)):\n",
    "    ax = fig.add_subplot(2, 4, j+1, xticks=[], yticks=[])\n",
    "    ax.imshow(x_train[i], cmap=plt.cm.binary, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 [2pts]: Parametric Classification: Using each of the 64 input features separately as the single input dimension, use parametric classification, assuming that the input is distributed according to a Gaussian. Report the training and test errors for the case of each of the 64 features. Which feature(s) give the best test performance? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates mean of the data\n",
    "def mean(data):\n",
    "    return data.sum()/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates standard deviation of the data\n",
    "def stddev(data):\n",
    "    avg = mean(data)\n",
    "    variance =  sum([(x-avg)**2 for x in data]) / float(len(data)-1)\n",
    "    return sqrt(variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and standard deviation for each column and length of each class in the data\n",
    "def parameters_1D(df):\n",
    "    parameters = {}\n",
    "    for col in list(df.columns)[:-1]:\n",
    "        feature_class6, feature_class9 = df[col][df[65] == 6], df[col][df[65] == 9]\n",
    "        feature_mean_class6, feature_mean_class9 = mean(feature_class6), mean(feature_class9)\n",
    "        feature_stddev_class6, feature_stddev_class9 = stddev(feature_class6), stddev(feature_class9)\n",
    "        # drop columns with zero standard devaition\n",
    "        if(feature_stddev_class6 == 0 or feature_stddev_class9 == 0):\n",
    "            print(\"Dropping column\",col)\n",
    "            continue\n",
    "        parameters.update({col:[\n",
    "            [feature_mean_class6, feature_mean_class9], \n",
    "            [feature_stddev_class6, feature_stddev_class9], \n",
    "            [len(feature_class6), len(feature_class9)]\n",
    "        ]})\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates Gaussian discriminant when mean and standard deviation is given using\n",
    "# g(x) = -(1/2)*log(2*pi) - log(stddev) - ((x-mean)^2)/(2*stddev^2) - log(prior)\n",
    "def discriminant_1D(x, mean, stddev, prior):\n",
    "    return -(1/2)*log(2*pi) - log(stddev) - ((x-mean)**2)/(2*stddev**2) - log(prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the class for each row using each of the column and return it in a dictionary class_predicted\n",
    "def predict_class_1D(df, parameters):\n",
    "     # create a dictionary of class predicted using each of the 64 features\n",
    "    class_predicted = {}\n",
    "    for col in parameters.keys():\n",
    "        feature_mean_class6, feature_mean_class9 = parameters[col][0][0], parameters[col][0][1]\n",
    "        feature_stddev_class6, feature_stddev_class9 = parameters[col][1][0], parameters[col][1][1]\n",
    "        predicted = []\n",
    "        # calculate prior probabilities\n",
    "        prior_class6, prior_class9 = parameters[col][2][0]/float(len(df)), parameters[col][2][1]/float(len(df))\n",
    "        for index, row in df.iterrows():\n",
    "            # calculate discriminant\n",
    "            disc_class6 = discriminant_1D(row[col], feature_mean_class6, feature_stddev_class6, prior_class6)\n",
    "            disc_class9 = discriminant_1D(row[col], feature_mean_class6, feature_stddev_class9, prior_class9)\n",
    "            predicted.append(6) if disc_class6 > disc_class9 else predicted.append(9)\n",
    "        class_predicted[col] = predicted\n",
    "    return class_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate error percentage\n",
    "def error_rate(actual, predicted):\n",
    "    wrong = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] != predicted[i]:\n",
    "            wrong += 1\n",
    "    return wrong / float(len(actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping column 1\n",
      "Dropping column 2\n",
      "Dropping column 7\n",
      "Dropping column 8\n",
      "Dropping column 9\n",
      "Dropping column 15\n",
      "Dropping column 16\n",
      "Dropping column 17\n",
      "Dropping column 22\n",
      "Dropping column 23\n",
      "Dropping column 24\n",
      "Dropping column 25\n",
      "Dropping column 32\n",
      "Dropping column 33\n",
      "Dropping column 40\n",
      "Dropping column 41\n",
      "Dropping column 48\n",
      "Dropping column 49\n",
      "Dropping column 57\n"
     ]
    }
   ],
   "source": [
    "params_1D = parameters_1D(train_df)\n",
    "train_class_predicted = predict_class_1D(train_df, params_1D)\n",
    "test_class_predicted = predict_class_1D(test_df, params_1D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 3 train error rate: 0.36\n",
      "Column 4 train error rate: 0.38\n",
      "Column 5 train error rate: 0.375\n",
      "Column 6 train error rate: 0.315\n",
      "Column 10 train error rate: 0.31\n",
      "Column 11 train error rate: 0.27\n",
      "Column 12 train error rate: 0.39\n",
      "Column 13 train error rate: 0.515\n",
      "Column 14 train error rate: 0.1\n",
      "Column 18 train error rate: 0.315\n",
      "Column 19 train error rate: 0.525\n",
      "Column 20 train error rate: 0.41\n",
      "Column 21 train error rate: 0.185\n",
      "Column 26 train error rate: 0.475\n",
      "Column 27 train error rate: 0.39\n",
      "Column 28 train error rate: 0.535\n",
      "Column 29 train error rate: 0.155\n",
      "Column 30 train error rate: 0.065\n",
      "Column 31 train error rate: 0.095\n",
      "Column 34 train error rate: 0.65\n",
      "Column 35 train error rate: 0.085\n",
      "Column 36 train error rate: 0.275\n",
      "Column 37 train error rate: 0.3\n",
      "Column 38 train error rate: 0.52\n",
      "Column 39 train error rate: 0.32\n",
      "Column 42 train error rate: 0.67\n",
      "Column 43 train error rate: 0.06\n",
      "Column 44 train error rate: 0.71\n",
      "Column 45 train error rate: 0.54\n",
      "Column 46 train error rate: 0.505\n",
      "Column 47 train error rate: 0.295\n",
      "Column 50 train error rate: 0.475\n",
      "Column 51 train error rate: 0.24\n",
      "Column 52 train error rate: 0.2\n",
      "Column 53 train error rate: 0.365\n",
      "Column 54 train error rate: 0.48\n",
      "Column 55 train error rate: 0.175\n",
      "Column 56 train error rate: 0.4\n",
      "Column 58 train error rate: 0.465\n",
      "Column 59 train error rate: 0.31\n",
      "Column 60 train error rate: 0.295\n",
      "Column 61 train error rate: 0.36\n",
      "Column 62 train error rate: 0.26\n",
      "Column 63 train error rate: 0.735\n",
      "Column 64 train error rate: 0.44\n"
     ]
    }
   ],
   "source": [
    "for col in train_class_predicted.keys():\n",
    "    print(\"Column\", col, \"train error rate:\", error_rate(list(train_df[65]), train_class_predicted[col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 3 test error rate: 0.22714681440443213\n",
      "Column 4 test error rate: 0.41274238227146814\n",
      "Column 5 test error rate: 0.43213296398891965\n",
      "Column 6 test error rate: 0.30193905817174516\n",
      "Column 10 test error rate: 0.25761772853185594\n",
      "Column 11 test error rate: 0.2631578947368421\n",
      "Column 12 test error rate: 0.2659279778393352\n",
      "Column 13 test error rate: 0.5152354570637119\n",
      "Column 14 test error rate: 0.08033240997229917\n",
      "Column 18 test error rate: 0.27146814404432135\n",
      "Column 19 test error rate: 0.4792243767313019\n",
      "Column 20 test error rate: 0.4376731301939058\n",
      "Column 21 test error rate: 0.14958448753462603\n",
      "Column 26 test error rate: 0.5069252077562327\n",
      "Column 27 test error rate: 0.34349030470914127\n",
      "Column 28 test error rate: 0.518005540166205\n",
      "Column 29 test error rate: 0.15789473684210525\n",
      "Column 30 test error rate: 0.1329639889196676\n",
      "Column 31 test error rate: 0.1329639889196676\n",
      "Column 34 test error rate: 0.6232686980609419\n",
      "Column 35 test error rate: 0.07479224376731301\n",
      "Column 36 test error rate: 0.2299168975069252\n",
      "Column 37 test error rate: 0.23268698060941828\n",
      "Column 38 test error rate: 0.4376731301939058\n",
      "Column 39 test error rate: 0.3573407202216066\n",
      "Column 42 test error rate: 0.6426592797783933\n",
      "Column 43 test error rate: 0.10249307479224377\n",
      "Column 44 test error rate: 0.7091412742382271\n",
      "Column 45 test error rate: 0.6177285318559557\n",
      "Column 46 test error rate: 0.5013850415512465\n",
      "Column 47 test error rate: 0.3767313019390582\n",
      "Column 50 test error rate: 0.44321329639889195\n",
      "Column 51 test error rate: 0.27146814404432135\n",
      "Column 52 test error rate: 0.16620498614958448\n",
      "Column 53 test error rate: 0.4487534626038781\n",
      "Column 54 test error rate: 0.47368421052631576\n",
      "Column 55 test error rate: 0.2659279778393352\n",
      "Column 56 test error rate: 0.4515235457063712\n",
      "Column 58 test error rate: 0.4598337950138504\n",
      "Column 59 test error rate: 0.2299168975069252\n",
      "Column 60 test error rate: 0.3545706371191136\n",
      "Column 61 test error rate: 0.407202216066482\n",
      "Column 62 test error rate: 0.31024930747922436\n",
      "Column 63 test error rate: 0.6565096952908587\n",
      "Column 64 test error rate: 0.4930747922437673\n"
     ]
    }
   ],
   "source": [
    "test_errors = {}\n",
    "i = 0\n",
    "for col in test_class_predicted.keys():\n",
    "    test_errors.update({col: error_rate(list(test_df[65]), test_class_predicted[col])})\n",
    "    print(\"Column\", col, \"test error rate:\", test_errors[col])\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 [2.5pts]: Use all the 64 features, assume that inputs are 64 dimensional Gaussians, and assume that for each class the covariance matrix is different. Report the training and test confusion matrices and errors. Hint: eliminate features that have variance zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataframe into 2 for the two classes 6 and 9. Also drop the columns with zero standard deviation or variance\n",
    "def split_frame(df):\n",
    "    X1, X2 = df.loc[df[65]==6], df.loc[df[65]==9]\n",
    "    X1, X2 = X1.loc[:, X1.columns!=65], X2.loc[:, X2.columns!=65]\n",
    "    # calculate standard deviation for each class\n",
    "    stddevX1, stddevX2 = X1.apply(np.std), X2.apply(np.std)\n",
    "    # drop columns with zero variance or standard deviation\n",
    "    newcols = []\n",
    "    for stddev1, stddev2, col in zip(stddevX1, stddevX2, X1.columns):\n",
    "        if stddev1 != 0 and stddev2 != 0:\n",
    "            newcols.append(col)\n",
    "        else:\n",
    "            print(\"Dropping column\", col)\n",
    "    return X1[newcols], X2[newcols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean, covariance and prior for the multivariate data\n",
    "def parameters_multivariate(df):\n",
    "    X1, X2 = split_frame(df)\n",
    "    # calculate mean of each class\n",
    "    meanX1, meanX2 = X1.apply(np.mean, axis=0), X2.apply(np.mean, axis=0)\n",
    "    # calculate covariance of each class\n",
    "    covarianceX1, covarianceX2 = np.cov(X1.T, bias = False), np.cov(X2.T, bias = False)\n",
    "    # calculate prior of each class\n",
    "    priorX1, priorX2 = len(X1)/float(len(df)), len(X2)/float(len(X2))\n",
    "    return {\n",
    "        \"Class 6\": [meanX1, covarianceX1, priorX1],\n",
    "        \"Class 9\": [meanX2, covarianceX2, priorX2],\n",
    "        \"cols\": list(X1.columns)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates discriminant for multivariate data when mean and covariance is given and covariance is different for each class\n",
    "# Mahalabonis distance = (x-mu)^T * sigma^-1 * (x-mu)\n",
    "# posterior p(x|Ci) = (1/(2*pi)**d/2 * |sigma|) * exp(-(1/2) * Mahalabonis distance)\n",
    "# discriminant g(x) = log(p(x|Ci)) + log(prior) = x.T*Wi*x + wi.T*x + wi0\n",
    "# where, Wi = -sigma^(-1)/2\n",
    "#        wi = sigma^(-1)*mui\n",
    "#        wi0 = -(mui.T*sigma^(-1)*mui)/2 - log(det(sigma))/2 - log(prior)\n",
    "def discriminant_multivariate(x, mean, covariance, prior):\n",
    "    inv_covariance = np.linalg.inv(covariance)\n",
    "    det_covariance = np.linalg.det(covariance)\n",
    "    Wi = -(1/2)*(inv_covariance)\n",
    "    wi = np.dot(inv_covariance, mean)\n",
    "    wi0 = -(1/2)*np.dot((mean.T), np.dot(inv_covariance, mean)) - (1/2)*log(abs(det_covariance)) - log(prior)\n",
    "    return np.dot((x.T), np.dot(Wi, x)) + np.dot((wi.T), x) + wi0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the class for given row using all the columns and return it in variable predicted\n",
    "def predict_class_multivariate(df, parameters):\n",
    "    predicted = []\n",
    "    # model parameters\n",
    "    mean_class6, mean_class9 = parameters[\"Class 6\"][0], parameters[\"Class 9\"][0]\n",
    "    cov_class6, cov_class9 = parameters[\"Class 6\"][1], parameters[\"Class 9\"][1]\n",
    "    prior_class6, prior_class9 = parameters[\"Class 6\"][2], parameters[\"Class 9\"][2]\n",
    "    cols = parameters[\"cols\"]\n",
    "    for index,row in df.iterrows():\n",
    "        # calculate multivariate discriminant\n",
    "        disc_class6 = discriminant_multivariate(row[cols], mean_class6, cov_class6, prior_class6)\n",
    "        disc_class9 = discriminant_multivariate(row[cols], mean_class9, cov_class9, prior_class9)\n",
    "        predicted.append(6) if disc_class6 > disc_class9 else predicted.append(9)\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping column 1\n",
      "Dropping column 2\n",
      "Dropping column 7\n",
      "Dropping column 8\n",
      "Dropping column 9\n",
      "Dropping column 15\n",
      "Dropping column 16\n",
      "Dropping column 17\n",
      "Dropping column 22\n",
      "Dropping column 23\n",
      "Dropping column 24\n",
      "Dropping column 25\n",
      "Dropping column 32\n",
      "Dropping column 33\n",
      "Dropping column 40\n",
      "Dropping column 41\n",
      "Dropping column 48\n",
      "Dropping column 49\n",
      "Dropping column 57\n"
     ]
    }
   ],
   "source": [
    "# Predict class for training data\n",
    "params_multivariate = parameters_multivariate(train_df)\n",
    "train_class_predicted_multivariate = predict_class_multivariate(train_df, params_multivariate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class for test data\n",
    "test_class_predicted_multivariate = predict_class_multivariate(test_df, params_multivariate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrices for training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted    6    9\n",
      "Actual             \n",
      "6          100    0\n",
      "9            0  100\n"
     ]
    }
   ],
   "source": [
    "train_data = {'y_Actual': list(train_df[65]), 'y_Predicted': train_class_predicted_multivariate}\n",
    "df_train = pd.DataFrame(train_data, columns=['y_Actual','y_Predicted'])\n",
    "confusion_matrix = pd.crosstab(df_train['y_Actual'], df_train['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])\n",
    "print (confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted    6    9\n",
      "Actual             \n",
      "6          176    5\n",
      "9            0  180\n"
     ]
    }
   ],
   "source": [
    "test_data = {'y_Actual': list(test_df[65]), 'y_Predicted': test_class_predicted_multivariate}\n",
    "df_test = pd.DataFrame(test_data, columns=['y_Actual','y_Predicted'])\n",
    "confusion_matrix = pd.crosstab(df_test['y_Actual'], df_test['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])\n",
    "print (confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error rates for training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error rate: 0.0\n",
      "Test Error rate: 0.013850415512465374\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Error rate:\", error_rate(list(train_df[65]), train_class_predicted_multivariate))\n",
    "print(\"Test Error rate:\", error_rate(list(test_df[65]), test_class_predicted_multivariate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 [1.5pts]: Repeat Q2, assuming that all the class covariance matrices are the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean, covariance and prior for the multivariate data with common covariance\n",
    "def parameters_multivariate_common_covariance(df):\n",
    "    X1, X2 = split_frame(df)\n",
    "    # calculate mean of each class\n",
    "    meanX1, meanX2 = X1.apply(np.mean, axis=0), X2.apply(np.mean, axis=0)\n",
    "    # calculate covariance of each class\n",
    "    covarianceX1, covarianceX2 = np.cov(X1.T, bias = False), np.cov(X2.T, bias = False) \n",
    "    # Common sample covariance S = sum over all i (prior of class i * covariance of class i)\n",
    "    common_covariance = (len(X1)/float(len(df)) * covarianceX1) + (len(X2)/float(len(df)) * covarianceX2)\n",
    "    # calculate prior of each class\n",
    "    priorX1, priorX2 = len(X1)/float(len(df)), len(X2)/float(len(X2))\n",
    "    return {\n",
    "        \"Class 6\": [meanX1, common_covariance, priorX1],\n",
    "        \"Class 9\": [meanX2, common_covariance, priorX2],\n",
    "        \"cols\": list(X1.columns)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates discriminant for multivariate data when mean and covariance is given and covariance is same for each class\n",
    "# discriminant g(x) = log(p(x|Ci)) + log(prior) = wi.T*x + wi0\n",
    "# where, wi = sigma^(-1)*mui\n",
    "#        wi0 = -(mui.T*sigma^(-1)*mui)/2 + log(prior)\n",
    "def discriminant_multivariate_common_covariance(x, mean, covariance, prior):\n",
    "    inv_covariance = np.linalg.inv(covariance)\n",
    "    det_covariance = np.linalg.det(covariance)\n",
    "    wi = np.dot(inv_covariance, mean)\n",
    "    wi0 = -(1/2)*np.dot((mean.T), np.dot(inv_covariance, mean)) + log(prior)\n",
    "    return np.dot((wi.T), x) + wi0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the class for given row using all the columns and return it in variable predicted\n",
    "def predict_class_multivariate_common_covariance(df, parameters):\n",
    "    predicted = []\n",
    "    mean_class6, mean_class9 = parameters[\"Class 6\"][0], parameters[\"Class 9\"][0]\n",
    "    cov_class6, cov_class9 = parameters[\"Class 6\"][1], parameters[\"Class 9\"][1]\n",
    "    prior_class6, prior_class9 = parameters[\"Class 6\"][2], parameters[\"Class 9\"][2]\n",
    "    cols = parameters[\"cols\"]\n",
    "    for index,row in df.iterrows():\n",
    "        # calculate multivariate discriminant\n",
    "        disc_class6 = discriminant_multivariate_common_covariance(row[cols], mean_class6, cov_class6, prior_class6)\n",
    "        disc_class9 = discriminant_multivariate_common_covariance(row[cols], mean_class9, cov_class9, prior_class9)\n",
    "        predicted.append(6) if disc_class6 > disc_class9 else predicted.append(9)\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping column 1\n",
      "Dropping column 2\n",
      "Dropping column 7\n",
      "Dropping column 8\n",
      "Dropping column 9\n",
      "Dropping column 15\n",
      "Dropping column 16\n",
      "Dropping column 17\n",
      "Dropping column 22\n",
      "Dropping column 23\n",
      "Dropping column 24\n",
      "Dropping column 25\n",
      "Dropping column 32\n",
      "Dropping column 33\n",
      "Dropping column 40\n",
      "Dropping column 41\n",
      "Dropping column 48\n",
      "Dropping column 49\n",
      "Dropping column 57\n"
     ]
    }
   ],
   "source": [
    "# Predict class for training data\n",
    "params_multivariate_common_cov = parameters_multivariate_common_covariance(train_df)\n",
    "train_class_predicted_mult_common_cov = predict_class_multivariate_common_covariance(train_df, params_multivariate_common_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class for test data\n",
    "test_class_predicted_mult_common_cov = predict_class_multivariate_common_covariance(test_df, params_multivariate_common_cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrices for training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted    6    9\n",
      "Actual             \n",
      "6          100    0\n",
      "9            0  100\n"
     ]
    }
   ],
   "source": [
    "train_data = {'y_Actual': list(train_df[65]), 'y_Predicted': train_class_predicted_mult_common_cov}\n",
    "df_train = pd.DataFrame(train_data, columns=['y_Actual','y_Predicted'])\n",
    "confusion_matrix = pd.crosstab(df_train['y_Actual'], df_train['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])\n",
    "print (confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted    6    9\n",
      "Actual             \n",
      "6          180    1\n",
      "9            0  180\n"
     ]
    }
   ],
   "source": [
    "test_data = {'y_Actual': list(test_df[65]), 'y_Predicted': test_class_predicted_mult_common_cov}\n",
    "df_test = pd.DataFrame(test_data, columns=['y_Actual','y_Predicted'])\n",
    "confusion_matrix = pd.crosstab(df_test['y_Actual'], df_test['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])\n",
    "print (confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error rates for training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error rate: 0.0\n",
      "Test Error rate: 0.002770083102493075\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Error rate:\", error_rate(list(train_df[65]), train_class_predicted_mult_common_cov))\n",
    "print(\"Test Error rate:\", error_rate(list(test_df[65]), test_class_predicted_mult_common_cov))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 [1.5pts]: Use the first 10 features in Q1 that gave the best test performance and repeat Q2. Compare the test performance you got to Q2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean, covariance and prior of the data\n",
    "def parameters_top10(df, cols):\n",
    "    X1, X2 = split_frame(df)\n",
    "    X1, X2 = X1.loc[:, X1.columns.isin(cols)], X2.loc[:, X2.columns.isin(cols)]\n",
    "    # calculate mean of each class\n",
    "    meanX1, meanX2 = X1.apply(np.mean, axis=0), X2.apply(np.mean, axis=0)\n",
    "    # calculate covariance of each class\n",
    "    covarianceX1, covarianceX2 = np.cov(X1.T, bias = False), np.cov(X2.T, bias = False)\n",
    "    # calculate prior of each class\n",
    "    priorX1, priorX2 = len(X1)/float(len(df)), len(X2)/float(len(X2))  \n",
    "    return {\n",
    "        \"Class 6\": [meanX1, covarianceX1, priorX1],\n",
    "        \"Class 9\": [meanX2, covarianceX2, priorX2],\n",
    "        \"cols\": list(X1.columns)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 features (with minimum error rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{35: 0.07479224376731301,\n",
       " 14: 0.08033240997229917,\n",
       " 43: 0.10249307479224377,\n",
       " 30: 0.1329639889196676,\n",
       " 31: 0.1329639889196676,\n",
       " 21: 0.14958448753462603,\n",
       " 29: 0.15789473684210525,\n",
       " 52: 0.16620498614958448,\n",
       " 3: 0.22714681440443213,\n",
       " 36: 0.2299168975069252}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_test_errors = {k: v for k, v in sorted(test_errors.items(), key=lambda item: item[1])[:10]}\n",
    "sorted_test_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping column 1\n",
      "Dropping column 2\n",
      "Dropping column 7\n",
      "Dropping column 8\n",
      "Dropping column 9\n",
      "Dropping column 15\n",
      "Dropping column 16\n",
      "Dropping column 17\n",
      "Dropping column 22\n",
      "Dropping column 23\n",
      "Dropping column 24\n",
      "Dropping column 25\n",
      "Dropping column 32\n",
      "Dropping column 33\n",
      "Dropping column 40\n",
      "Dropping column 41\n",
      "Dropping column 48\n",
      "Dropping column 49\n",
      "Dropping column 57\n"
     ]
    }
   ],
   "source": [
    "# Predict class for training data\n",
    "params_top10 = parameters_top10(train_df, list(sorted_test_errors.keys()))\n",
    "train_class_predicted_top10 = predict_class_multivariate(train_df, params_top10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class for test data\n",
    "test_class_predicted_top10 = predict_class_multivariate(test_df, params_top10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrices for training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted    6    9\n",
      "Actual             \n",
      "6          100    0\n",
      "9            0  100\n"
     ]
    }
   ],
   "source": [
    "train_data = {'y_Actual': list(train_df[65]), 'y_Predicted': train_class_predicted_top10}\n",
    "df_train = pd.DataFrame(train_data, columns=['y_Actual','y_Predicted'])\n",
    "confusion_matrix = pd.crosstab(df_train['y_Actual'], df_train['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])\n",
    "print (confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted    6    9\n",
      "Actual             \n",
      "6          172    9\n",
      "9            0  180\n"
     ]
    }
   ],
   "source": [
    "test_data = {'y_Actual': list(test_df[65]), 'y_Predicted': test_class_predicted_top10}\n",
    "df_test = pd.DataFrame(test_data, columns=['y_Actual','y_Predicted'])\n",
    "confusion_matrix = pd.crosstab(df_test['y_Actual'], df_test['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])\n",
    "print (confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error rates for training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error rate: 0.0\n",
      "Test Error rate: 0.024930747922437674\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Error rate:\", error_rate(list(train_df[65]), train_class_predicted_top10))\n",
    "print(\"Test Error rate:\", error_rate(list(test_df[65]), test_class_predicted_top10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test performance as compared to Q2 decline since the error rate increases which means only selecting top 10 features misses some attributes of the data that contributes to the prediction of class and hence reducing the accuracy of our model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
